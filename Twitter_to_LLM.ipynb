{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NcAEQx2nIhJP",
        "q1FB8zk-8Qh0",
        "UsvtVFjLGThD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SametOzenc/mc/blob/master/Twitter_to_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openaikey=\"x\"\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"]=openaikey\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=\"x\"\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"x\""
      ],
      "metadata": {
        "id": "XmAgyODzGYpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Twitter Profile"
      ],
      "metadata": {
        "id": "NcAEQx2nIhJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install ntscraper\n",
        "from ntscraper import Nitter\n",
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "username=input(\"\")\n",
        "\n",
        "\n",
        "tweets = Nitter().get_tweets(f\"{username}\", mode='user', number=100)\n",
        "with open(f\"{username}.json\", \"w\") as file:\n",
        "    json.dump(tweets, file, indent=4)\n",
        "\n",
        "\n",
        "# ---------------------Save data---------------------\n",
        "\n",
        "with open(f\"{username}.json\", 'r') as file:\n",
        "    json_data = json.load(file)\n",
        "df = pd.json_normalize(json_data['tweets'])\n",
        "\n",
        "df.to_csv(f'{username}.csv', index=False)\n",
        "print(\"CSV file saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "-HWMD3PVIkgO",
        "outputId": "c4c8b44b-6fff-4e06-fc16-eda9e6c16dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-f067d6fd1057>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0musername\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "q1FB8zk-8Qh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_genai\n",
        "!pip install PyPDF2\n",
        "!pip install langchain\n",
        "!pip install faiss-cpu #gpu\n",
        "!pip install langchain_openai"
      ],
      "metadata": {
        "id": "speyRMFdo3pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJl3XwnaorE0"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.document_loaders import DataFrameLoader\n",
        "import pandas as pd\n",
        "\n",
        "df=pd.read_csv(f\"{username}.csv\")\n",
        "seelcted_columns=[\"text\",\"date\",\"quoted-post.text\"]\n",
        "df_new=df[seelcted_columns]\n",
        "\n",
        "loader = DataFrameLoader(df_new, page_content_column=\"text\")\n",
        "\n",
        "def get_text_chunks(text):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "chunks=get_text_chunks(str(loader.load()))\n",
        "vector_store = FAISS.from_texts(chunks, OpenAIEmbeddings())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_retrieval_response\",\n",
        "            \"description\": \"Get responses from documents, texts, files, pdf's etc\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"user_input\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Query about the documents\",\n",
        "                    },\n",
        "                    \"doc_data\":{\n",
        "                        \"type\":\"string\",\n",
        "                        \"description\": \"data related to the user input\",\n",
        "                    },\n",
        "\n",
        "                },\n",
        "                \"required\": [\"user_input\",\"index_number\"]\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "3Rv77cN_qAsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_retrieval_response(prompt):\n",
        "  query = prompt\n",
        "  docs = vector_store.similarity_search(query)\n",
        "  return docs"
      ],
      "metadata": {
        "id": "5HdHrVRx-svL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "def get_reponse(prompt):\n",
        "  completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-0125\",\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are my helpful Twitter LLM (Large Language Model), your name is TLMM\"},\n",
        "      {\"role\": \"user\", \"content\":  f\"{prompt}\"}\n",
        "    ],\n",
        "    tools=tools\n",
        "  )\n",
        "\n",
        "  if completion.choices[0].message.content!=None:\n",
        "    print(completion.choices[0].message.content)\n",
        "  #print(completion.choices[0].message.tool_calls)\n",
        "  if completion.choices[0].message.tool_calls!=None:\n",
        "    #print(\"Function is running...\")\n",
        "    docs=get_retrieval_response(prompt)\n",
        "    completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-0125\",\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are my helpful Twitter LLM (Large Language Model), your name is TLMM\"},\n",
        "      {\"role\": \"user\", \"content\":  f\"Use the following pieces of context to answer the user's question. If you can't find the answer in context, just say that you don't know, don't try to make up an answer. {prompt} in this context: {docs}.\"}\n",
        "    ]\n",
        "  )\n",
        "    print(completion.choices[0].message.content)\n",
        "\n"
      ],
      "metadata": {
        "id": "mbZNjLBmxW5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "UsvtVFjLGThD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  prompt=input(\"\")\n",
        "  if prompt!=\"exit\":\n",
        "    query = prompt\n",
        "    docs = vector_store.similarity_search(query)\n",
        "    get_reponse(prompt)\n",
        "\n",
        "  else:\n",
        "    break"
      ],
      "metadata": {
        "id": "ACeMM9XPGUh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "982cde0d-387d-42ff-a142-5d2c311a16b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hi who are you\n",
            "Hello! I am TLMM, your helpful Twitter LLM (Large Language Model). How can I assist you today?\n",
            "what are the most recent topics in context\n",
            "The most recent topics in the given context are related to discussions about freedom of speech, censorship, government-industrial complex, disinformation, birth control pill side effects, powerwall technology, and SpaceX achievements.\n",
            "thoughts on freedom of speech \n",
            "In the context provided, there are different viewpoints presented on freedom of speech. One post mentions, \"Free speech is the bedrock of democracy. That’s why it’s the FIRST Amendment. Without free speech, all is lost.\" This indicates a strong support for the importance of freedom of speech in maintaining democracy.\n",
            "\n",
            "On the other hand, there is a post discussing concerns about government interference in censorship of free speech, highlighting a legal battle where it's claimed that the Biden administration coerced social media companies into suppressing free speech on various topics. This post expresses a concern about the misuse of power to censor certain views.\n",
            "\n",
            "Ultimately, the context reflects a range of perspectives on freedom of speech, from emphasizing its critical role in democracy to raising concerns about potential censorship and government influence on free speech rights.\n",
            "thanks\n",
            "You're welcome! If you have any questions or need assistance, feel free to ask.\n",
            "exit\n"
          ]
        }
      ]
    }
  ]
}